我想做一个大模型召回能力测试框架，名字叫 hogwarts-bench ，顾名思义我会把哈利波特系列小说作为内容塞给大模型，测试它的召回水平。

程序分为两部分，一个是测试题生成器，一个是测试工具本身

## 首先，测试题生成器的构想：
使用python 的 OpenAI 库，默认接入 openrouter 接口，用户通过.env 文件指定接口，模型等LLM参数。
程序支持并发，在参数中支持并发数量调整。
程序接受一个参数叫novel用来读取小说纯文本文件，另一个参数是question_nums, 指定生成测试题的数量, 输入novel后需要使用python随机抽取小说的段落，注意是段落，建议先按照换行符在全文中按照question_nums，随机选择段落， 尽量使段落自然的分布在小说中的各处，然后扩展段落的前500字和后500字作为上下文，然后通过调用AI整理出来当前的段落大意，并生成正面和反面问题，这里需要一个提示词模板配置，需要设计为有默认值，然后可以手动修改的配置。然后运行的时候，比如塞给大模型哈利波特小说第一部，然后选中了下面的文本：

```

确实，自从海格说漏嘴以后，他们一直在书里寻找勒梅的名字，除此之外，他们还有什么办法可以弄清斯内普想偷的是什么东西呢？麻烦的是，他们很难知道从何处入手，不知道勒梅有什么突出成就，能够被写进书里。他不在《二十世纪的大巫师》里，也不在《当代著名魔法家名录》里。另外，《现代魔法的重大发现》和《近代巫术发展研究》中也找不到他的名字。还有，当然啦，单是馆内藏书的规模就令人望而却步，那里有成千上万本书，几千个书架，几百条狭窄的通道。赫敏从口袋里掏出一张清单，上面列着她决定要查找的主题和书名。与此同时，罗恩在一排图书前溜达着，漫无目标地把一些书从书架上面抽出来。哈利不知不觉来到禁书区。不幸的是，要查找任何一本禁书都必须有某位老师亲笔签名的纸条，哈利知道他是不可能弄到这种纸条的。在这些书里，包含着从不在霍格沃茨课堂上讲授的很厉害的黑魔法，只有高年级学生在研究高深的“黑魔法防御术”时才能读到。
```

然后需要大模型生成问题和回答的结构化数据：

```
{"question":"自从海格说漏嘴以后，哈利在寻找有关尼克勒梅的信息中，他们至少寻找了 《二十世纪的大巫师》《当代著名魔法家名录》《现代魔法的重大发现》和哪本书?", "question_type":"single_choice" "choice": {"a":"《近代巫术发展研究》", "b":"《霍格沃兹一段校史》", "c":"《高等巫术》", "d":"《高等炼金术》"}, "answer":"a", "position":"132455"}
```

其中 question 是问题, question_type 是问题类型，包含 single_choice 和 mutiple_choice, choice 是选项, answer 是正确答案, position 是当前选定的问题的原文位于小说的 token 位置, 比如小说全文相当于400K token, 我们选中的内容直到内容结束位于 132455 token. 这里我们默认使用 openai 的 tokenizer 进行计算.





## 测试工具

测试工具也是用 python OpenAI 库，参数同样有 novel用来给大模型提供小说。

程序支持并发，在参数中支持并发数量调整。

然后还有个参数 context_length 用来标记本次测试的测试召回范围, 需要使用 openai 的 tokenizer 计算原文后进行切割。比如原文是400k，用户指定 context_length 为20k，这时候需要把原文切割为20K。从头部开始切割即可。

第三个参数是 data_set, 用来指定本次测试使用的问题集，也就是我们的测试题生成器生成的问题，由于默认需要指定context_length，我们会遍历问题集选择 position + 500 < context_length 内的问题，注意需要留500 token 的 padding. 每个问题单独提问大模型。即每次提问都需要把小说和问题重新发送给大模型。

然后把切割后的小说和问题塞给大模型，这里还是需要一个提示词模板，好让大模型回答问题。这里需要让大模型尽可能输出结构化数据方便找到特征进行匹配, 比如让大模型输出固定模板 ```{"answer":["a"]}```. 然后遍历文本匹配到左大括号后"{"开始记录答案, 匹配到右大括号后停止"}" 这里用正则表达式也行。然后解析json并得到大模型的回答。注意由于存在多选题，所以answer的值类型是数组。

最后整理大模型的回答输出结构化数据报告。这里也提供参数来指定输出地址。

## 分析工具

接受测试工具输出的报告，计算大模型的精确率召回率即 Recall = Relevant retrieved instances / All relevant instances. 然后生成网页报告，报告要包含召回率表格，和一个横轴chart，横坐标轴是0到 context_length，然后上面的散点通过颜色区别命中，未命中或者错误命中。








### **项目名称：hogwarts-bench**

#### **项目定位**
一个专为评测大语言模型（LLM）长文本召回与理解能力的自动化测试框架。该框架以《哈利·波特》系列小说为基准语料库，通过“大海捞针”式的提问，系统性地评估模型在不同上下文长度、不同信息位置下的事实查找、细节回忆与信息综合能力。

项目由三个核心模块构成：**测试题生成器**、**自动化测试工具**和**分析与报告生成器**。

---

### **第一部分：测试题生成器 (Test Question Generator)**

**核心目标：** 从原始小说文本中，自动化、高质量地生成结构化的测试题库。

**功能详述：**

1.  **基础配置：**
    *   使用 Python 的 `OpenAI` 库，默认通过 `.env` 文件配置 LLM 参数（如 API Key, Base URL, Model Name），并默认接入 OpenRouter 接口以支持多种模型。
    *   支持通过命令行参数调整并发数量，以加速题目生成过程。

2.  **输入参数：**
    *   `--novel`: 指定要读取的小说纯文本文件路径。
    *   `--question_nums`: 指定本次要生成的测试题总数。
    *   `--sampling_strategy`: (可选) 抽样策略，默认为 `stratified`。
    *   `--context_window_size`: (可选) 生成问题时，为LLM提供的上下文窗口大小，默认为 `500` 字。

3.  **段落抽样与上下文构建：**
    *   程序首先使用 `tiktoken` 库对全文进行分词。
    *   **抽样策略：**
        *   **随机抽样 (`random`)：** 在小说全文中随机选择 `question_nums` 个段落起点。
        *   **分层抽样 (`stratified`)：** (推荐) 将小说按章节或固定Token数（如每5万Token）分层，在每层中均匀抽取问题，确保测试题覆盖小说的所有部分。
    *   **上下文构建：** 对于每个抽样点，提取其前后的 `context_window_size` 个Token作为上下文，发送给LLM。

4.  **智能问题生成：**
    *   **提示词模板 (Prompt Template)：** 内置一套高效的提示词模板，并支持用户通过配置文件进行自定义。该模板将引导LLM生成多样化且高质量的问题。
    *   **问题类型多样性：**
        *   **单选题 (single_choice):** 侧重于对单一、精确事实的召回。
        *   **多选题 (multiple_choice):** 重点考察模型的“查全率”(Recall)，即找出所有相关信息的能力。
        *   **否定性问题 (negative_questions):** 生成一些原文明确否定或从未提及的“陷阱”问题，用以评估模型的幻觉水平和事实辨别能力。

5.  **结构化输出与校验：**
    *   要求LLM输出严格的JSON格式数据。
    *   **输出数据结构:**
        ```json
        {
          "question": "自从海格说漏嘴以后，哈利在寻找有关尼克·勒梅的信息中，他们至少寻找了 《二十世纪的大巫师》《当代著名魔法家名录》《现代魔法的重大发现》和哪本书?",
          "question_type": "single_choice",
          "choice": {
            "a": "《近代巫术发展研究》",
            "b": "《霍格沃兹一段校史》",
            "c": "《高等巫术》",
            "d": "《高等炼金术》"
          },
          "answer": ["a"], // 答案统一为数组，兼容单选和多选
          "position": {
             "start_pos": 132455, // 答案在原文中的起始Token位置
             "end_pos": 132490    // 答案在原文中的结束Token位置
          }
        }
        ```
    *   **质量控制：** 程序在接收到LLM的输出后，会自动进行一次**格式校验**（是否为合法JSON）和**内容校验**（`answer` 是否是 `choice` 的有效子集），校验失败的题目将被舍弃或触发重试。

---

### **第二部分：自动化测试工具 (Testing Tool)**

**核心目标：** 使用生成的问题集，在指定的上下文长度下，对目标LLM进行自动化评测。

**功能详述：**

1.  **基础配置：**
    *   同样使用 Python 的 `OpenAI` 库，并支持并发调用。

2.  **输入参数：**
    *   `--novel`: 评测使用的小说原文。
    *   `--data_set`: 测试题生成器产出的问题集文件路径（JSONL格式）。
    *   `--context_length`: 本次测试的上下文长度（以Token计），例如 `20000`。
    *   `--context_strategy`: (可选) 上下文切割策略，默认为 `first_n`。
    *   `--padding_size`: (可选) 确保问题答案区域不被截断的Token缓冲大小，默认为 `500`。

3.  **上下文处理与问题过滤：**
    *   **上下文切割策略：**
        *   `first_n` (头部截取): 从小说开头截取 `context_length` 个Token。
        *   `last_n` (尾部截取): 从小说结尾向前截取 `context_length` 个Token。
        *   `middle_n` (中部截取): 在小说中部随机位置截取 `context_length` 个Token。
        *   `needle_in_haystack` (大海捞针模式): 将包含答案的段落（针）随机插入到一个很长但不相关的文本（干草堆）中，构成 `context_length` 的上下文。
    *   **问题过滤：** 遍历问题集，只选择 `position.end_pos + padding_size < context_length` 的问题进行测试。

4.  **测试执行与答案解析：**
    *   对于每一个筛选出的问题，程序将“切割后的上下文”和“问题”一同发送给目标LLM。
    *   **提示词模板：** 提供一个独立的测试提示词模板，强制要求LLM以 `{"answer": ["a", "d"]}` 的JSON格式回答问题。
    *   **答案解析（多级降落策略）：**
        1.  **首选：** 尝试直接用 `json.loads()` 解析模型的完整输出。
        2.  **备选：** 若解析失败，使用正则表达式 `r'\{.*\}'` 提取JSON字符串，再次尝试解析。
        3.  **最终：** 若仍然失败，将该问题的回答标记为 **“解析失败 (Parsing Error)”**。

5.  **结构化输出：**
    *   将每次问答的结果（包含原始问题、正确答案、模型回答、解析状态等）保存为结构化的输出文件（如JSONL），供分析工具使用。

---

### **第三部分：分析与报告生成器 (Analysis and Report Generator)**

**核心目标：** 对测试结果进行深度分析，并生成直观、可交互的网页报告。

**功能详述：**

1.  **输入：** 接收测试工具输出的结果文件。

2.  **核心指标计算：**
    *   **单选题：** 计算整体的 **准确率 (Accuracy)**。
    *   **多选题：** 对每个问题计算 **精确率 (Precision)**、**召回率 (Recall)** 和 **F1-Score**，并最终报告这些指标的宏平均值 (Macro Average)。

3.  **网页报告生成：**
    *   生成一份独立的、可交互的 HTML 报告，包含以下模块：
    *   **A. 总体性能摘要 (Summary)：**
        *   测试配置（模型、上下文长度、策略等）。
        *   核心指标汇总表格（Accuracy, Avg. Precision, Avg. Recall, Avg. F1-Score）。
        *   总问题数、有效测试数、解析失败数、拒绝回答数等统计。
    *   **B. 召回性能散点图 (Recall Performance Scatter Plot)：**
        *   **横坐标 (X-axis):** 答案在上下文中的Token位置 (`position.start_pos`)，范围从 0 到 `context_length`。
        *   **散点颜色区分状态：**
            *   **绿色：** 完全正确 (Accuracy=1 或 F1-Score=1)。
            *   **黄色：** 部分正确 (例如，多选题回答了部分正确答案，0 < F1 < 1)。
            *   **红色：** 完全错误 (Accuracy=0 或 F1-Score=0)。
            *   **灰色：** 解析失败或模型拒绝回答。
        *   **交互性：** 使用 `Plotly` 或 `ECharts` 库实现。鼠标悬停在任何一个点上时，显示该问题的详细信息（问题、正确答案、模型答案、得分）。
        *   **趋势线：** 在图上叠加一条平滑的趋势线，直观展示模型性能是否随信息位置的深化而衰减。
    *   **C. 错误案例分析 (Error Analysis)：**
        *   随机展示若干个回答错误或部分正确的案例，方便进行人工分析和调试。

