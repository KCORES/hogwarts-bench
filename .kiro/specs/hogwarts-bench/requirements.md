# 需求文档

## 简介

Hogwarts-bench 是一个自动化测试框架，专门用于评估大语言模型（LLM）的长文本召回和理解能力。该系统以《哈利·波特》系列小说作为基准语料库，采用"大海捞针"的方法，系统性地评估模型在不同上下文长度和信息位置下的事实检索、细节回忆和信息综合能力。该框架由三个核心模块组成：测试题生成器、自动化测试工具和分析报告生成器。

## 术语表

- **系统（System）**: hogwarts-bench 框架
- **题目生成器（Question Generator）**: 负责从小说文本生成测试题的模块
- **测试工具（Testing Tool）**: 对目标 LLM 执行测试的模块
- **报告生成器（Report Generator）**: 分析测试结果并生成报告的模块
- **LLM**: 被评测的大语言模型
- **上下文窗口（Context Window）**: 围绕采样点提取的 token 范围，用于题目生成
- **上下文长度（Context Length）**: 测试时提供给 LLM 的总 token 数
- **采样策略（Sampling Strategy）**: 从小说中选择文本片段的方法
- **题目集（Question Set）**: JSONL 格式的生成测试题集合
- **Token 位置（Token Position）**: 文本在小说中以 token 为单位的位置
- **填充大小（Padding Size）**: 确保答案区域不被截断的缓冲 token 数
- **测试结果（Test Result）**: 包含问题、正确答案、模型回答和评估指标的输出

## 需求

### 需求 1: 从小说文本生成测试题

**用户故事：** 作为基准测试创建者，我希望能够从小说文本自动生成结构化的测试题，以便无需手动工作即可创建全面的题目集。

#### 验收标准

1. WHEN 用户提供小说文件路径和题目数量时，THE 题目生成器 SHALL 读取小说文本并使用 tiktoken 进行分词
2. WHEN 分词完成后，THE 题目生成器 SHALL 应用指定的采样策略来选择文本片段
3. WHEN 未指定采样策略时，THE 题目生成器 SHALL 使用分层采样作为默认策略
4. WHERE 选择分层采样时，THE 题目生成器 SHALL 按章节或固定 token 数将小说分层，并从每层均匀提取问题
5. WHERE 选择随机采样时，THE 题目生成器 SHALL 在整个小说中随机选择指定数量的片段起始点

### 需求 2: 上下文感知的题目生成

**用户故事：** 作为基准测试创建者，我希望题目能够在适当的上下文中生成，以便问题有意义且可回答。

#### 验收标准

1. WHEN 采样到文本片段时，THE 题目生成器 SHALL 根据上下文窗口大小提取采样点前后的 token
2. WHEN 未指定上下文窗口大小时，THE 题目生成器 SHALL 使用 500 个 token 作为默认值
3. WHEN 提取上下文时，THE 题目生成器 SHALL 将边界对齐到句子或段落分隔符以保持语义完整性
4. WHEN 上下文准备好后，THE 题目生成器 SHALL 将其与提示词模板一起发送给 LLM
5. WHEN LLM 生成题目后，THE 题目生成器 SHALL 记录答案在原始小说中的 token 位置

### 需求 3: 多样化的题目类型生成

**用户故事：** 作为基准测试创建者，我希望生成多种类型的题目，以便全面评估不同的模型能力。

#### 验收标准

1. WHEN 生成题目时，THE 题目生成器 SHALL 支持单选题类型以测试精确的事实召回
2. WHEN 生成题目时，THE 题目生成器 SHALL 支持多选题类型以测试召回完整性
3. WHEN 生成题目时，THE 题目生成器 SHALL 支持否定性题目类型以测试幻觉检测
4. WHEN 生成多选题时，THE 题目生成器 SHALL 在提示词模板中要求至少 2 个干扰选项
5. WHEN 生成任何题目类型时，THE 题目生成器 SHALL 使用配置的提示词模板或内置的默认模板

### 需求 4: 结构化题目输出和验证

**用户故事：** 作为基准测试创建者，我希望生成的题目遵循严格的格式，以便下游工具能够可靠地处理它们。

#### 验收标准

1. WHEN LLM 生成题目时，THE 题目生成器 SHALL 要求 JSON 格式输出，包含 question、question_type、choice、answer 和 position 字段
2. WHEN 接收到 LLM 输出时，THE 题目生成器 SHALL 验证输出是否为有效的 JSON
3. WHEN 接收到 LLM 输出时，THE 题目生成器 SHALL 验证 answer 字段仅包含 choice 字段中的有效选项
4. WHEN 验证失败时，THE 题目生成器 SHALL 根据 retry_times 参数丢弃题目或重试生成
5. WHEN 所有题目生成完成后，THE 题目生成器 SHALL 将题目集保存到 JSONL 文件，并包含元数据（生成时间戳、模型名称和配置参数）

### 需求 5: 并发题目生成

**用户故事：** 作为基准测试创建者，我希望能够并发生成题目，以便减少总生成时间。

#### 验收标准

1. WHEN 用户指定并发参数时，THE 题目生成器 SHALL 使用指定数量的并发工作器生成题目
2. WHEN 并发生成处于活动状态时，THE 题目生成器 SHALL 管理 API 调用以避免超过速率限制
3. WHEN 生成请求失败时，THE 题目生成器 SHALL 重试最多 retry_times 指定的次数
4. WHEN 未指定 retry_times 时，THE 题目生成器 SHALL 使用 3 作为默认重试次数
5. WHEN 所有重试都用尽后，THE 题目生成器 SHALL 记录失败并继续处理剩余题目

### 需求 6: LLM 配置管理

**用户故事：** 作为基准测试创建者，我希望能够配置 LLM 连接参数，以便使用不同的模型和 API 提供商。

#### 验收标准

1. WHEN 题目生成器启动时，THE 系统 SHALL 从 .env 文件加载 LLM 配置
2. WHEN 加载配置时，THE 系统 SHALL 读取 API 密钥、基础 URL 和模型名称参数
3. WHEN 未提供配置时，THE 系统 SHALL 使用 OpenRouter 作为默认 API 端点
4. WHEN 进行 API 调用时，THE 系统 SHALL 使用 Python OpenAI SDK
5. WHEN 进行 API 调用时，THE 系统 SHALL 在请求中包含所有配置的参数

### 需求 7: 自动化测试执行

**用户故事：** 作为模型评估者，我希望使用生成的题目自动测试 LLM，以便衡量其长文本性能。

#### 验收标准

1. WHEN 用户提供小说文件、题目集文件和上下文长度时，THE 测试工具 SHALL 加载小说并进行分词
2. WHEN 分词完成后，THE 测试工具 SHALL 根据指定的上下文长度提取前 N 个 token
3. WHEN 上下文准备好后，THE 测试工具 SHALL 过滤答案位置加填充大小小于上下文长度的题目
4. WHEN 未指定填充大小时，THE 测试工具 SHALL 使用 500 个 token 作为默认值
5. WHEN 题目过滤完成后，THE 测试工具 SHALL 将每个题目与准备好的上下文一起发送给目标 LLM

### 需求 8: 答案解析和错误处理

**用户故事：** 作为模型评估者，我希望有健壮的答案解析功能，以便处理各种 LLM 响应格式。

#### 验收标准

1. WHEN LLM 返回响应时，THE 测试工具 SHALL 尝试使用 json.loads 将其解析为 JSON
2. IF json.loads 失败，THEN THE 测试工具 SHALL 使用正则表达式提取 JSON 并重试解析
3. IF 正则提取失败，THEN THE 测试工具 SHALL 将响应标记为解析错误
4. WHEN 解析成功时，THE 测试工具 SHALL 从 JSON 中提取 answer 字段
5. WHEN 所有题目测试完成后，THE 测试工具 SHALL 将结果保存到 JSONL 文件，包含问题、正确答案、模型答案和解析状态

### 需求 9: 测试结果分析

**用户故事：** 作为模型评估者，我希望获得详细的性能指标，以便了解模型的优势和劣势。

#### 验收标准

1. WHEN 报告生成器接收到测试结果时，THE 系统 SHALL 计算单选题的准确率
2. WHEN 报告生成器接收到测试结果时，THE 系统 SHALL 为每个多选题计算精确率、召回率和 F1 分数
3. WHEN 计算多选题指标时，THE 系统 SHALL 计算所有题目的宏平均值
4. WHEN 计算完成后，THE 系统 SHALL 生成包含总题目数、有效测试数、解析失败数和拒绝回答数的摘要
5. WHEN 生成指标时，THE 系统 SHALL 将每个结果分类为正确、部分正确、错误或解析错误

### 需求 10: 交互式 HTML 报告生成

**用户故事：** 作为模型评估者，我希望获得交互式可视化报告，以便轻松探索和理解测试结果。

#### 验收标准

1. WHEN 生成报告时，THE 报告生成器 SHALL 创建包含嵌入式 JavaScript 的独立 HTML 文件
2. WHEN 打开报告时，THE 系统 SHALL 显示包含测试配置和核心指标的摘要部分
3. WHEN 打开报告时，THE 系统 SHALL 显示散点图，X 轴为 token 位置，Y 轴为性能
4. WHEN 显示散点图时，THE 系统 SHALL 将点颜色编码为：绿色表示正确，黄色表示部分正确，红色表示错误，灰色表示解析错误
5. WHEN 用户悬停在散点上时，THE 系统 SHALL 显示题目详情，包括题目文本、正确答案、模型答案和分数

### 需求 11: 性能趋势可视化

**用户故事：** 作为模型评估者，我希望看到跨上下文位置的性能趋势，以便识别模型是否存在位置偏差。

#### 验收标准

1. WHEN 生成散点图时，THE 报告生成器 SHALL 叠加平滑的趋势线
2. WHEN 计算趋势线时，THE 系统 SHALL 使用移动平均或多项式回归
3. WHEN 显示趋势线时，THE 系统 SHALL 使其在视觉上与散点区分开
4. WHEN 报告包含趋势线时，THE 系统 SHALL 允许用户切换其可见性
5. WHEN 生成可视化时，THE 系统 SHALL 使用 Plotly 或 ECharts 库实现交互性

### 需求 12: 错误案例分析

**用户故事：** 作为模型评估者，我希望查看具体的错误案例，以便调试和改进评估过程。

#### 验收标准

1. WHEN 生成报告时，THE 报告生成器 SHALL 包含错误分析部分
2. WHEN 显示错误案例时，THE 系统 SHALL 随机选择可配置数量的错误或部分正确的示例
3. WHEN 显示每个错误案例时，THE 系统 SHALL 显示题目、正确答案、模型答案和计算的分数
4. WHEN 显示错误案例时，THE 系统 SHALL 以可读的布局格式化它们
5. WHEN 用户查看错误案例时，THE 系统 SHALL 提供足够的上下文以理解答案为何错误

### 需求 13: 提示词模板管理

**用户故事：** 作为基准测试创建者，我希望能够自定义提示词模板，以便针对不同场景优化题目生成和测试。

#### 验收标准

1. WHEN 题目生成器启动时，THE 系统 SHALL 从配置文件加载提示词模板
2. WHEN 提示词模板配置文件不存在时，THE 系统 SHALL 使用内置的默认模板
3. WHEN 生成题目时，THE 系统 SHALL 使用题目生成提示词模板
4. WHEN 测试模型时，THE 系统 SHALL 使用要求 JSON 格式答案的测试提示词模板
5. WHEN 提示词模板更新后，THE 系统 SHALL 在下次生成或测试运行时应用更改
