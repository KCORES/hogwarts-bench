# Hogwarts-bench Configuration Example
# Copy this file to .env and fill in your values

# ============================================================================
# LLM Configuration
# ============================================================================

# OpenAI API Key (required)
# Get your API key from OpenAI or your API provider
OPENAI_API_KEY=your_api_key_here

# OpenAI Base URL (optional)
# Default: https://openrouter.ai/api/v1
# For OpenAI: https://api.openai.com/v1
# For other providers: use their API endpoint
OPENAI_BASE_URL=https://openrouter.ai/api/v1

# Model Name (required)
# Examples:
#   - OpenRouter: anthropic/claude-3-sonnet, openai/gpt-4-turbo
#   - OpenAI: gpt-4-turbo-preview, gpt-3.5-turbo
MODEL_NAME=anthropic/claude-3-sonnet

# ============================================================================
# Generation Settings
# ============================================================================

# Temperature for LLM generation (0.0 - 2.0)
# Lower values make output more deterministic
# Default: 0.7
DEFAULT_TEMPERATURE=0.7

# Top-p / Nucleus Sampling (0.0 - 1.0)
# Only considers tokens with cumulative probability <= top_p
# Lower values = more focused, higher values = more diverse
# Example: 0.9 means only top 90% probability mass is considered
# Default: 1.0 (disabled, all tokens considered)
DEFAULT_TOP_P=1.0

# Top-k Sampling (0 or positive integer)
# Only considers the top k most likely tokens at each step
# Lower values = more focused, higher values = more diverse
# Example: 40 means only top 40 tokens are considered
# Default: 0 (disabled, all tokens considered)
DEFAULT_TOP_K=0

# Maximum tokens in LLM response
# Default: 2000
DEFAULT_MAX_TOKENS=2000

# Request timeout in seconds
# Default: 300
DEFAULT_TIMEOUT=300

# ============================================================================
# Concurrency Settings
# ============================================================================

# Default number of concurrent API requests
# Adjust based on your API rate limits
# Default: 5
DEFAULT_CONCURRENCY=5

# Default number of retry attempts for failed requests
# Default: 3
DEFAULT_RETRY_TIMES=3

# Default delay between retry attempts (in seconds)
# Default: 5
DEFAULT_RETRY_DELAY=5

# Interactive retry prompt
# When enabled, prompts user to retry after all retry attempts are exhausted
# Set to true/1/yes to enable, default is disabled
# INTERACTIVE_RETRY=false

# ============================================================================
# Optional: Advanced Settings
# ============================================================================

# Custom User-Agent header for API requests
# Some APIs (like Kimi For Coding) require specific User-Agent to work
# Example: KimiCLI/0.1.0
# USER_AGENT=

# Enable thinking/reasoning output (for models that support it like Kimi)
# Set to true/1/yes to enable, default is disabled
# ENABLE_THINKING=false

# Thinking style for different API providers
# Options: openai (default), kimi
# - openai: Uses {"enable_thinking": true} in extra_body (Qwen3, etc.)
# - kimi: Uses {"thinking": {"type": "enabled"}} in extra_body
# THINKING_STYLE=openai

# Tokenizer encoding name
# Default: cl100k_base (used by GPT-4 and GPT-3.5-turbo)
# TOKENIZER_ENCODING=cl100k_base

# Log level (DEBUG, INFO, WARNING, ERROR)
# LOG_LEVEL=INFO
